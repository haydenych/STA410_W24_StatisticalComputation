{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YymWs_lQA0_x"
   },
   "source": [
    "<a name=\"cell-solving\"></a>\n",
    "\n",
    "### 3. [$Ax=b$](#first_cell) [and Singular Values](#first_cell)\n",
    "\n",
    "1. [Vector and Matrix Norms](#cell-sovling-condition-norms)\n",
    "\n",
    "2. [Condition of](#cell-sovling-condition-derivation) [$x=f_{A}(b)=A^{-1}b$](#cell-sovling-condition-derivation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpqAeaJzrjRC"
   },
   "source": [
    "<a name=\"cell-sovling-condition-norms\"></a>\n",
    "\n",
    "## Vector and Matrix Norms ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "For $f(x) = Ax$ the statement $f(x) \\approx f(x+\\epsilon_x)$ means that $y = f(x) - f(x+\\epsilon_x) \\approx 0$ which is judged on the basis of the ***norm*** (or magnitude, or size) of the ***vector*** $y$, notated as $||y||$. The most common ***vector norms*** are the ubiquetous $L_p$ ***norms***\n",
    "\n",
    "$$||x||_p = \\left(\\sum_i |x_i|^{p}\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "$$||x||_2 = \\underset{L_2 \\text{: Euclidean}}{\\sqrt{\\sum_i x_i^2}} \\quad   \\quad ||x||_1 = \\underset{L_1 \\text{: Manhattan}}{\\sum_i |x_i|} \\quad  \\quad ||x||_\\infty = \\underset{L_\\infty \\text{: Chebyshev}}{\\max_i |x_i|}$$\n",
    "\n",
    "- ***Norms*** will be considered again in the context of function (vector) spaces. Some additional considerations regarding ***norms*** are available in Keith Knight's STA410 [notes8.pdf](https://q.utoronto.ca/courses/296804/files?preview=24301082) document. \n",
    "\n",
    "We can also define the ***norm*** (or magnitude, or size) of a matrix $A$.  One common method to do so is to induce a ***matrix norm*** from the $L_p$ ***vector norms*** as \n",
    "\n",
    "$$||A||_p = \\underset{x\\not=0}{\\max} \\frac{||Ax||_p}{||x||_p}$$\n",
    "\n",
    "Another common ***matrix norm*** is the ***Frobenius norm***\n",
    "$$||A||_F = \\left(\\sum_i\\sum_j A_{ij}^2\\right)^{\\frac{1}{2}} =  \\underbrace{\\sqrt{\\text{tr}(AA^T)} = \\sqrt{\\text{tr}(A^TA)} }_{\\text{trace doesn't care about transposes}} = \\underset{\\text{singular values of } A}{\\overset{\\text{The $L_2$ norm of the}}{\\sqrt{ \\sum_i \\lambda_i^2}}}$$\n",
    "\n",
    "which is just a direct extension of $L_2$ ***Euclidean distance***. \n",
    "\n",
    "- Further analysis of ***condition*** in the context of $L_p$ ***induced matrix norms*** and of $L_p$ ***induced matrix norms*** themselves (e.g., using ***Manhattan*** and ***Chebyshev distance*** ***induced matrix norms*** to derive bounds on the ***spectral radius***) is available in Keith Knight's STA410 [notes8.pdf](https://q.utoronto.ca/courses/296804/files?preview=24301082) document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm-GiZM3uZVM"
   },
   "source": [
    "<a name=\"cell-sovling-condition-derivation\"></a>\n",
    "\n",
    "## Condition of $x=f_{A}(b)=A^{-1}b$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "When computing a solultion $x=f_{A}(b)=A^{-1}b$ to $Ax = b$ for an invertible linear transformation (***square full rank***) $A$, either\n",
    "\n",
    "$$\\underset{\\Large \\text{$f_A$ is well-conditioned}}{b+\\epsilon_b \\approx b \\Longrightarrow f_A(b+\\epsilon_b) \\approx f_A(b)} \\quad \\text{ or } \\quad \\underset{\\Large \\text{$f_A$ is ill-conditioned}}{b+\\epsilon_b \\approx b \\cancel{\\Longrightarrow} f_A(b+\\epsilon_b) \\approx f_A(b)}$$\n",
    "\n",
    "Since actual value $\\tilde x = [A^{-1}b]_c$ obtainable in the ${\\rm I\\!F}$ ***floating-point*** representation of ${\\rm I\\!R}$ will actually be a solution to a different problem \n",
    "\n",
    "$$A \\tilde x = \\tilde b \\quad \\text{with} \\quad \\tilde x = x + \\epsilon_x \\; \\text{and}\\; \\tilde b = b + \\epsilon_b.$$\n",
    "\n",
    "then, for any reasonable ***vector norm*** $||\\cdot||$ measuring magnitude, $A$ is called ***well-conditioned*** if \n",
    "\n",
    "- whenever $\\frac{||\\epsilon_b||}{||b||}$ is small $\\quad\\quad\\quad\\quad\\quad\\quad\\;\\;$  whenever the \"nearby problem\" actually being solved is \"close\"\n",
    "- $\\Longrightarrow$ then $\\frac{||\\epsilon_x||}{||x||}$ is also small $\\quad\\quad\\quad\\quad\\quad\\;\\Longrightarrow$ the solution is also \"close\"\n",
    "\n",
    "\n",
    "So a matrix $A$ is ***well-conditioned*** with respect to $x = f_A(b) = A^{-1}b$ if small changes in the input $b$ to the function, do not produces large changes in the output $x$.  Contrarily, $A$ is ***ill-conditioned*** if $f_A(b) = A^{-1}b$ is highly volatile relative to small changes in the input $b$.\n",
    "\n",
    "The ***condition*** of $A$ in $f_A(b) = A^{-1}b$ can actually be given a precise numeric quantification on the basis of the ***induced matrix norm*** $||A|| = \\underset{x\\not=0}{\\max} \\frac{||Ax||}{||x||}$ since\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{it implies } && ||b|| = {} &  ||Ax|| \\leq ||A|| \\; ||x|| \\\\\n",
    "\\text{and thus } && \\frac{1}{||x||} \\leq {} & \\frac{||A||}{||b||} \\\\ \\\\\n",
    "\\text{and since } && \\epsilon_x = {} & A^{-1} \\epsilon_b \\quad \\text{ (by the definition of $\\epsilon_x$ and $\\epsilon_b$)},\\\\\n",
    "&&||\\epsilon_x|| = {} & ||A^{-1} \\epsilon_b|| \\leq {} ||A^{-1}|| \\; ||\\epsilon_b||\\\\\\\\\n",
    "\\text{the product } &&\\frac{||\\epsilon_x||}{||x||} \\leq {} & \\underbrace{||A||\\;||A^{-1}||}_{\\Large \\kappa(A)}\\;\\frac{||\\epsilon_b||}{||b||} \\quad \\text{follows}\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "Thus the ***condition number*** of $A$ for the problem of solving for $x$ in $Ax=b$ is defined as \n",
    "\n",
    "|$\\Large \\kappa(A) = ||A||\\;||A^{-1}||\\quad $ | [$\\Large\\quad  1 \\leq \\kappa(A) < \\infty$](https://math.stackexchange.com/questions/4116544/any-example-of-condition-number-of-matrix-less-than-1)|\n",
    "|-|-|\n",
    "|||\n",
    "\n",
    "and explicitly bounds how small $\\frac{||\\epsilon_x||}{||x||}$ will be relative to $\\frac{||\\epsilon_b||}{||b||}$, characterizing how rapidly the output of the function $x = f(b) = A^{-1}b$ can change for small changes in the input $b$. Large $\\kappa(A)$ means $\\frac{||\\epsilon_x||}{||x||}$ may not be small even if $\\frac{||\\epsilon_b||}{||b||}$ is small, so $A$ is ***well-conditioned*** if $\\kappa(A)$ is small and ***ill-conditioned*** otherwise. \n",
    "\n",
    "\n",
    "\n",
    "> For the $L_2$ ***induced matrix norm***, and a ***symmetric real valued*** matrix $A$ (which thus has ***real eigenvalues***)\n",
    "> \n",
    "> \\begin{align*}\n",
    "||A||_2 = {} & \\underset{x\\not=0}{\\max} \\frac{||Ax||_2}{||x||_2}\n",
    "= \\underset{||x||_2=1}{\\max} \\frac{||A(cx)||_2}{||(cx)||_2} \n",
    "= \\underset{||x||_2=1}{\\max} \\frac{|c|||Ax||_2}{|c|||x||_2} \n",
    "= \\underset{||x||_2=1}{\\max} ||Ax||_2 \\\\\n",
    "= {} & \\underset{i}{\\max} |\\lambda_i| \\quad \\text{ (the largest magnitude eigenvalue for square $A$)}\\\\\n",
    "\\color{gray}{=} {} & \\color{gray}{\\sqrt{\\rho(A^TA)} \\quad \\text{ (the square root of the spectral radius of the gramian of non-square $A$)}} \n",
    "\\end{align*}\n",
    ">\n",
    "> and for ***symmetric*** and ***diagonizable*** ([***normal***](https://en.wikipedia.org/wiki/Normal_matrix)) $A$ with a real-valued ***orthogonal eigendecomposition*** \n",
    ">\n",
    "> $$\\begin{align*}\n",
    "||A^{-1}||_2  = {} & ||\\overset{\\text{eigendecom-}}{\\overbrace{(V\\Lambda V^T)}^{\\text{position of } A}} {}^{-1}||_2  = ||V\\Lambda^{-1}V^T||_2  \\\\\n",
    " = {} & \\frac{1}{|\\lambda_\\min^A|} \\quad \\text{ (the reciprocal of the smallest magnitude eigenvalues of $A$)}\\end{align*}$$\n",
    "> Thus, for the $L_2$ ***induced matrix norm***, the ***condition number*** $\\kappa(A) = ||A||_2\\;||A^{-1}||_2$ depends on the relative magnitudes of the smallest and largest ***eigenvalues*** with\n",
    "> $$\\underset{\\text{the ratio of the largest and smallest eigenvalues}}{\\kappa(A) = ||A||_2 ||A^{-1}||_2 = \\frac{|\\lambda_\\max^A|}{|\\lambda_\\min^A|}} \\quad \\text{ and } \\quad \\frac{||\\epsilon_x||_2}{||x||_2} \\leq \\frac{|\\lambda_\\max^A|}{|\\lambda_\\min^A|}\\frac{||\\epsilon_b||_2}{||b||_2}$$\n",
    "\n",
    "\n",
    "<!--\n",
    "> Thus for ***square full rank*** $A$, the previously noted bound on the ***spectral radius***\n",
    ">\n",
    "> $$\\rho(A_{n\\times n}) = \\max_i |\\lambda_i| = ||A||_2 \\leq \\begin{array}{c} \\max_i \\sum_j |A_{ij}| \\\\ \\max_j \\sum_i |A_{ij}|\\end{array}$$\n",
    "> \n",
    "> follows since for ***eigenvalue*** and ***eigenvector*** pair $\\lambda_i$ and $v_i$, $|\\lambda_i| \\leq ||A||_p$ as seen from \n",
    "$$|\\lambda_i| = |\\lambda_i| \\,||v_i||_p = ||\\lambda_i v_i||_p = \\overbrace{||A v_i||_p \\leq ||A||_p ||v_i||_p}^{\\text{by definition of the induced norm}} = ||A||_p \\cancel{||v_i||_p}^1$$\n",
    ">\n",
    "> and the bounds are the ***Manhattan*** and ***Chebyshev distance*** ***induced matrix norms*** (as shown in Keith Knight's STA410 [class notes](https://q.utoronto.ca/courses/244990/files?preview=18669503))\n",
    ">\n",
    "> $$||A||_1=\\max_j \\sum_i |A_{ij}| \\quad \\text{ and } \\quad ||A||_\\infty =\\max_i \\sum_j |A_{ij}|$$\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kcu-1lFk8eMY"
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
