{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJxjIdFUxe5j"
   },
   "source": [
    "<a name=\"cell-solving\"></a>\n",
    "\n",
    "### 4. [Algorithmic Speed](#cell-sovling-Axb) / [Direct Methods](#cell-sovling-Axb) \n",
    "1. [Computational Speed](#cell-computational-speed)\n",
    "    1. [Optimized Operations](#cell-operation-optimization)\n",
    "    2. [Operation Counting](#cell-operation-counting)\n",
    "        - [Sequential summation VS the `fan` method](#cell-operation-counting-fan)\n",
    "    3. [\"Big O\" Algorithmic Complexity](#cell-sovling-bigO)\n",
    "        - [$O(n) \\text{ and } O(1)$](#cell-sovling-bigO-On-O1)\n",
    "        - [$O(n^2) \\text{ and } O(n^3)$](#cell-sovling-bigO-On2-On3)\n",
    "            - [A Counting Example with Convolutions](#cell-FPN-counting)\n",
    "        - [$O(n\\log(n)) \\text{ and } O(n^{\\log_2 7})$](#cell-sovling-bigO-Onlogn) \n",
    "            - [Fast Fourier Transform Preview](#cell-sovling-bigO-Onlogn)\n",
    "\n",
    "2. [Direct Methods finding](#cell-sovling-direct-methods) [$X\\hat \\beta \\approx y \\text{ by solving } \\overset{A\\, x}{X^TX \\hat \\beta} = \\overset{b}{X^T y}$](#cell-sovling-direct-methods): (and see also `STA410_W24_Week5_Extra_MoreLeastSquares.ipynb`)\n",
    "    1. [\"Big O\" Algorithmic Complexity](#cell-sovling-direct-methods-big0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZOZiMjqsS0nj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPWdJJzRxsne"
   },
   "source": [
    "<a name=\"cell-sovling-Axb\"></a>\n",
    "\n",
    "# 4 $Ax \\approx b$ / Algorithmic Speed ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "There is a often tradeoff between numerical precision and computational efficiency.\n",
    "\n",
    "> The `fahan` summation method and the recursive formula of the variance calculation from the `STA410_W24_Week4_Homework_AdditionVariance.ipynb` notebook were good examples of this.  They both performed extra numerical operations in an attempt to reduce ***roundoff error*** in the sequential operations.\n",
    "\n",
    "Before looking more closely at the tradeoff between numerical precision and computational efficiency, let's first consider computational efficiency alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUOJRvTRuBSd"
   },
   "source": [
    "<a name=\"cell-computational-speed\"></a>\n",
    "\n",
    "## 4.0 Computational Speed ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "There are three factors driving the computational speed of any algorithm:\n",
    "\n",
    "0. How much time does each actual operation in the algorithm take?\n",
    "1. How many such operations are required to complete the algorithm for a given problem input size?\n",
    "2. How do required operations scale as the problem size grows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsS4QkzDvkh0"
   },
   "source": [
    "<a name=\"cell-operation-optimization\"></a>\n",
    "\n",
    "## 4.1.0 Optimized Operations ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Following up on the Digital VS Analog Computing videos in the `STA410_W24_Week4_Extra_AnalogVsDigital_BitstringArithmatic_GracefulUnderflow.ipynb` notebook, it is exactly the case that computational hardware is built to perform specific operations and is then subsequently optimized for exactly that task. Subsequently, data types and operations on them which directly map onto optimized computational hardware processes can be extremely fast.\n",
    "\n",
    "This is demonstrated below with the `np.array` data type which can process certain operations up to 100 times faster than a `list` data type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80UZbyS0v2vG"
   },
   "outputs": [],
   "source": [
    "# a \"vector\" -- mapping directly onto \"vectorized\" hardware computations\n",
    "x_np_array = stats.norm().rvs(10000)\n",
    "# a \"linked list\" -- which must be traversed sequentially as A->B->C->D->etc.\n",
    "# and therefore does not simply map right onto a computational hardware operation\n",
    "n, x_list = len(x_np_array), list(x_np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJCP6n5UwPoz"
   },
   "outputs": [],
   "source": [
    "%timeit sum(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVP3F0MnwhCZ"
   },
   "outputs": [],
   "source": [
    "# ultra optimized\n",
    "%timeit x_np_array.sum()\n",
    "# ...doesn't do \"memory walk\" traversing a \"linked list\" like A->B->C->D->etc.\n",
    "# ...just gets mapped onto a computational hardware operation which then executes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3fWQJqww9L0"
   },
   "outputs": [],
   "source": [
    "# and look how optimized the `x_np_array.sum()` operation is compared to the following\n",
    "%timeit x_np_array.sum()/n\n",
    "# summing 10000 is so much faster than dividing by n..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3lrZKJ-wokO"
   },
   "outputs": [],
   "source": [
    "# Interestingly, x_np_array.sum() is about twice as fast as x_np_array.mean()\n",
    "%timeit x_np_array.mean()\n",
    "# Presumably x_np_array.mean() is dividing every number by n before summing them\n",
    "# which means it requires twice as many computations as x_np_array.sum()\n",
    "# but this will minimize roundoff overall by accumulating values closer to 0\n",
    "\n",
    "# This is another good example of the numerical accuracy / computational efficiency tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0shrf1sjy1PI",
    "outputId": "b19bf520-8a9e-4db9-a729-4f0a758d9086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 µs ± 2.39 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# so instead of x_np_array.sum() the following might be more accuarte\n",
    "%timeit x_np_array.mean()*n\n",
    "# but for the example data used here x_np_array.mean()*n == x_np_array.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXIsqUDW5iKY"
   },
   "source": [
    "<a name=\"cell-operation-counting\"></a>\n",
    "\n",
    "## 4.1.1 Operation Counting ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Numeric operations are one basic kind of operations performed in an algorithm. Accessing and placing data in memory is another.\n",
    "\n",
    "Recall the \"two pass\" and \"single pass\" sum of squares algorithms\n",
    "\n",
    "\n",
    "$$\\sum_{i=1}^n(x_i-\\bar x)^2 = \\underset{\\text{first computes } \\bar x}{\\underbrace{\\sum_{i=1}^n x_i^2 - n \\bar x^2}_{\\text{two pass algorithm}}} = \\underset{\\text{$\\sum_{i=1}^{k-1 \\rightarrow k} x_i^2$ and $\\sum_{i=1}^{k-1\\rightarrow k} x_i$ at step $k$}}{\\underbrace{\\sum_{i=1}^n x_i^2 - \\frac{1}{n} \\left(\\sum_{i=1}^n x_i\\right)^2}_{\\text{single pass algorithm increments}}}$$\n",
    "\n",
    "While the number of arithmetic operations required by each method is the same, the number of memory access operations required by each is different; namely, the two pass algorithm must access the data in memory *twice* as many times as the single pass algorithm. \n",
    "\n",
    "> The recursive formulation of the above calcutions does entail additional computations intended to minimize ***roundoff error***, which is more similar to the algorithm employed by `np.var()` function. So the `np.var()` function uses minimal memory access operations, but about twice as many numeric operations as the two \"naive\" algorithms above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEcGoVC8gPvg"
   },
   "outputs": [],
   "source": [
    "def ss_two_pass(x,n):\n",
    "  sumx2 = x[0]**2\n",
    "  for i in range(1, n):\n",
    "    sumx2 = sumx2 + x[i]**2\n",
    "  sumx = x[0]\n",
    "  for i in range(1, n):\n",
    "    sumx = sumx + x[i]\n",
    "  return sumx2-(sumx**2)/n\n",
    "\n",
    "def ss_single_pass(x,n):\n",
    "  sumx = x[0]\n",
    "  sumx2 = sumx**2\n",
    "  for i in range(1, n):\n",
    "    xi = x[i]\n",
    "    sumx2 = sumx2+xi**2\n",
    "    sumx = sumx+xi\n",
    "  return sumx2-(sumx**2)/n\n",
    "\n",
    "# just numerical operations without accessing memory\n",
    "def ss_numerical_operations(x,n):\n",
    "  sumx = x[0]\n",
    "  sumx2 = sumx**2\n",
    "  for i in range(1, n):\n",
    "    sumx2+sumx2**2\n",
    "    sumx+sumx\n",
    "  return sumx2-(sumx**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqysGty4llLT",
    "outputId": "195cc016-80cf-4225-be1e-a91b060bf374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.64 ms ± 53.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ss_two_pass(x_list,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XIYt2Y0ilv0",
    "outputId": "6afde09c-e678-457b-ec1a-03a2150b93a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3 ms ± 73 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ss_single_pass(x_list,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYIBLOn5psef",
    "outputId": "e7f10411-7a41-452e-a2f9-e3de5b32d0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ms ± 67.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ss_numerical_operations(x_list,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BpoOGpPnh6M",
    "outputId": "b3cd73c7-895b-4e32-d6da-66e84ea6f398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indeed, memory access is twice as slow in the two pass versus the one pass algorithms\n",
    "(4.6-4)/(4.3-4)\n",
    "# And also notice that the numerical operations themselves (~4 ms)\n",
    "# are less optimized than the memory access operations (~0.3 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrzeTLEv6zJQ"
   },
   "outputs": [],
   "source": [
    "# naive two pass algorithm with `x_np_array` like 100x faster than with `x_list`\n",
    "%timeit (x_np_array**2).sum() - x_np_array.sum()**2/len(x_np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a2AGLkPDCwh"
   },
   "outputs": [],
   "source": [
    "# single pass `np.var()` \"single pass recurrsive\" algorithm is twice as slow...\n",
    "%timeit x_np_array.var(ddof=0)*len(x_np_array)\n",
    "# because it uses more numerical operations to attempt to reduce roundoff error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtVrG_1y_-r"
   },
   "source": [
    "<a name=\"cell-operation-counting-fan\"></a>\n",
    "\n",
    "### The sequential summation versus the `fan` method ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "The `STA410_W24_Week4_Homework_AdditionVariance.ipynb` considered a sequential summation algorithm and the alternative `fan` algorithm. Convince yourself that these require the same number of additions. Now, consider the run times below. The difference is not due to a different number of additions since those is the same.  The difference is caused by the the fact that the `fan` requires memory in which to store the new list to pass to the next recursive call of the `fan` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7LFs2Y12TJ2"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1,2,2**15)\n",
    "\n",
    "def ss(x):\n",
    "  x_sum = 0\n",
    "  for xi in x:\n",
    "    x_sum += xi\n",
    "\n",
    "def fan(x):\n",
    "    '''assumes len(x)=2**p always'''\n",
    "    if len(x)==1:\n",
    "        return x\n",
    "    return fan([x[j*2]+x[j*2+1] for j in range(int(len(x)/2))])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIhrAZNB2Hbp"
   },
   "outputs": [],
   "source": [
    "%timeit ss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ_KX1MW2Feo"
   },
   "outputs": [],
   "source": [
    "%timeit fan(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qib1pSmh3IwJ"
   },
   "source": [
    "<a name=\"cell-sovling-bigO\"></a>\n",
    "\n",
    "## 4.1.2 \"Big O\" Algorithmic Complexity ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "An algorithm requiring $f(n)$ operations to complete a problem with input size $n$ has ***Big $O$*** ***algorithmic complexity*** order $g(n)$, written as \n",
    "\n",
    "$$f(n) = O(g(n)) \\quad \\text{ if } \\quad f(n) \\leq M g(n) \\quad \\text{ for all $n>n_0$}$$\n",
    "\n",
    "for strictly positive $g$ and some constant $M$ that is not a function of $n$.\n",
    "\n",
    "The \"***Big O***\" ***algorithmic complexity*** of an algorithm measures the *proportional* computational demands of the algorithm as a function of the input size $n$ of the problem.\n",
    "\n",
    "> While $f(n) = an^2+bn+c = O(n^2)$, the $O(n^2)$ only characterizes the computational characteristics of an algorithm in the context of very large $n$. \n",
    " Specifying \"***Big O***\" ***computational complexity*** is pretty simple and generally just requires dropping lower order terms and ignoring proportinality coefficients. The actual runtime of an algorithm for a given input size $n$ depends on $a$ and the lower order terms $bn+c$ but these are not characterized by $O(n^2)$ since as $n$ grows large $an^2+bn+c< Mn^2$ for some large $M$. \n",
    ">\n",
    "> $O(100n) = O(n) < O(n^2)$, but for $n=100$, the actual computational requirements for $f(n)=100n$ and $g(n)=n^2$ would be indistinguishable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bt2ev9oVJT9"
   },
   "source": [
    "<a name=\"cell-sovling-bigO-On-O1\"></a>\n",
    "\n",
    "### $O(n)$ and $O(1)$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "\"***Big O***\" ***algorithmic complexity*** only characterizes the long term cost of an algorithm as the input $n$ gets larger.  For example, the number of memmory reads for the two pass and single pass sum of squares have the same \"***Big O***\" ***algorithmic complexity*** \n",
    "\n",
    "$$SS_{2pass}^{\\#memreads}(n) = 2n = O(n) \\quad \\text{ and } \\quad SS_{1pass}^{\\#memreads}(n) = n = O(n)$$\n",
    "\n",
    "despite the fact that the former requires twice as many actual memory reading computations as the latter. \n",
    "\n",
    "\n",
    "The fact that an algorithm has a \"***Big O***\" ***algorithmic complexity*** $O(n)$ doesn't mean we know the specific computational requirements at a given problem input size $n$; rather, it characterizes that these computational requirements will scale ***linearly*** as a function of $n$.\n",
    "\n",
    "\n",
    "> As we demonstrated, the memory access costs of the \"two pass\" algorithm are twice those of the \"single pass\" algrithm, depite both algorthms having \"***Big O***\" ***algorithmic complexity*** $O(n)$.\n",
    ">\n",
    "> Similarly, the the overall computational requirements of \n",
    "> \n",
    "> - `ss_two_pass(x,len(x))` and `ss_single_pass(x,len(x))`\n",
    ">\n",
    "> are about 100 times that of \n",
    "> \n",
    "> - `x.var(ddof=0)*len(x)` \n",
    ">\n",
    "> which in turn requires about 2 times that of \n",
    ">\n",
    "> - `lambda x: (x**2).sum() - x.sum()**2/len(x) # x an np.array`\n",
    "> \n",
    "> and yet all of these algorithms have \"***Big O***\" ***algorithmic complexity*** $O(n)$.\n",
    "\n",
    "> Another good example that is helpful for understanding the nature of \"***Big O***\" ***algorithmic complexity*** is the case of $O(1)$, which means that the computational requirements do not change as a function of the size of the input data.\n",
    ">\n",
    "> - An example of $O(1)$ is storage requirements **beyond** an input buffer needed to calculate a running mean, sum, or variance from streaming data (since these computations all admit \"streaming\" one pass calculations).\n",
    ">    - An algorithm which simply needs to stream through an input buffer,  *even if it does so several times*, is called an ***online*** or **\"out of core\"** algorithm. This is why the \"two pass\" sum of squares algorithm was called an ***online*** algorithm.\n",
    "> - Median calculations do not admit an $O(1)$ storage requirement. The storage requirements of a median calculation are $O(n)$ because to determine which data point is the median the complete data set must be sorted in memory. \n",
    ">    - An algorithm with $O(n)$ storage requirements is called a ***batch*** algorithm and requires constant access to the input data.\n",
    "> - An ***online*** or **\"out of core\"** algorithm that requires only a *single* pass through an input buffer *and* only has an additional storage requirement of $O(1)$ is called a ***real-time*** algorithm.\n",
    ">    - Variance calculations using only a single pass of the data are ***real-time*** algorithms; and, covariance matrix calculations also admit ***real-time*** forms; and, linear regression model fitting in fact also has a ***real-time*** form (which doesn't even requires any covariance matrix calculations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3kMrjK6Miv7"
   },
   "source": [
    "<a name=\"cell-sovling-bigO-On2-On3\"></a>\n",
    "\n",
    "### $O(n^2)$ and $O(n^3)$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Examples of \"***Big O***\" ***algorithmic complexity*** $O(n^2)$ and $O(n^3)$ are matrix-vector multiplication and matrix-matrix multiplication, respectively. \n",
    "\n",
    "- matrix-vector multiplication $A_{n \\times n} x_{n \\times 1}$ requires $n^2$ multiplications and $n(n − 1)$ additions: $n^2 + n(n − 1) = O(n^2)$\n",
    "\n",
    "  > $10,000^2 = 100,000,000$ might be computationally tractable; but, $n^2$ for larger $n$ quickly starts to look computationally unappealing...\n",
    "\n",
    "- matrix-matrix multiplication $A_{n \\times n} B_{n \\times n}$ requires $n$ matrix-vector multiplication: $n\\times(n^2 + n(n − 1)) = O(n^3)$\n",
    "\n",
    "  > $1000^3 = 1,000,000,000$ *might* be computationally tractable; but, $n^3$ for larger $n$ immediately looks like a computational death trap...\n",
    "\n",
    "In terms of the problem input size $n$, it becomes quickly apparent that the computational requirements of \"***Big O***\" ***algorithmic complexity*** $O(n^2)$ and $O(n^3)$ algorithms scale tremendously differently as $n$ increases, and that neither is particularly attractive as a function of $n$ relative to \"***Big O***\" ***algorithmic complexity*** $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1ra60TIiNNU"
   },
   "source": [
    "<a name=\"cell-FPN-counting\"></a>\n",
    "\n",
    "#### A Counting Example with Convolutions ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Determining $O(f(n))$ is simply a matter of defining what's being counted, and then carefully counting it. \n",
    "\n",
    "> Consider the following (***convolution***) algorithm $f(n)$ computing the distribution of $p(X_1+\\cdots+X_n)$ for discrete $X_i \\overset{i.i.d.}{\\sim} p(X_i=x)$ with $x \\in \\{0, \\cdots, K\\}$:\n",
    ">\n",
    "> $$\\begin{align*}\n",
    "p(X_1+X_2 = s) = & {} \\sum_{x=0}^{\\min(s,K)} p(X_1=s-x) p(X_2=x)\\\\\n",
    "p(X_1+X_2+X_3 = s) = & {} \\sum_{x=0}^{\\min(s,K)} p(X_1+X_2 = s-x) p(X_3=x)  \\\\\n",
    "p(X_1+X_2+X_3+X_4 = s) = & {} \\sum_{x=0}^{\\min(s,K)} p(X_1+X_2+X_3 = s-x) p(X_4=x)  \\\\\n",
    "\\vdots  & {}\n",
    "\\end{align*}$$\n",
    "\n",
    "For this example some possible counting targets might be \n",
    "- How many multiplications are required to define $p(X_1+\\cdots+X_n)$?\n",
    "- How many additional additions are required to define $p(X_1+\\cdots+X_n)$?\n",
    "- How much storage is required to compute $p(X_1+\\cdots+X_n)$?\n",
    "\n",
    "> To answer the first counting question about multiplication, note that at each stage \n",
    ">  1. $\\quad p(X_1+X_2)$\n",
    ">  2. $\\quad p(X_1+X_2+X_3)$\n",
    ">\n",
    ">    $\\quad\\quad\\quad\\quad\\vdots$\n",
    "> \n",
    "> $\\quad$k. $\\quad p(X_1+\\cdots+X_k = s)$ must be calculated for $s \\in \\{0, \\cdots, kK\\}$\n",
    "> \n",
    "> and each \n",
    "> - $p(X_1+\\cdots+X_k = s)$ calculation requires the sum of $\\min(s,K)+1$ multiplications of $p(X_k)$ with the previous $p(X_1+\\cdots+X_{k-1})$, i.e.,\n",
    ">\n",
    ">   $$\\sum_{x=0}^{\\min(s,K)} \\underbrace{p(X_1+\\cdots+X_{k-1} = s-x)}_{\\text{can be cached as a time-space tradeoff}} p(X_k=x)$$\n",
    ">\n",
    ">   which can be cached (as a ***time-space tradeoff***) if the algorithm is approached in the natural sequential manner.\n",
    ">\n",
    "> Thus, the total number of multiplications required is \n",
    ">\n",
    "> $$\\begin{align*}\n",
    "\\overset{\\text{stage}}{\\sum_{k=2}^n} \\overbrace{\\sum_{s=0}^{kK} \\big(\\min(s,K)+1\\big)}^{\\text{stage $k$ multiplications}} & \\leq {} \\sum_{k=1}^n \\sum_{s=0}^{kK} \\big(K+1\\big)\\\\\n",
    "& = {} \\sum_{k=1}^n (kK+1) (K+1)\\\\\n",
    "O\\left(\\sum_{k=1}^n (kK+1) (K+1)\\right) & = {} O\\left(\\sum_{k=1}^n kK^2+kK + K+1 \\right)\\\\\n",
    "& = {} O\\left(\\sum_{k=1}^n kK^2\\right) = O\\left( \\frac{n(n+1)}{2}K^2\\right)\\\\\n",
    "& = {} O\\left(n^2K^2\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "> There is an alternative algorithmic approach for this problem based on the ***moment generating function***\n",
    "> \n",
    "> $$\\begin{align*}\n",
    "E\\left[t^S\\right] = {} & \\sum_{s=0}^{nK} t^s p(S=s) \\\\\n",
    "= E\\left[t^{X_1 + \\cdots + X_n}\\right] = {} &  \\underbrace{E\\left[\\prod_{i=1}^n t^{X_i}\\right] = \\prod_{i=1}^nE\\left[ t^{X_i}\\right]}_{\\text{since } X_1, \\cdots, X_n \\text{ are independent}}\\\\  \n",
    "= {} & \\left(\\sum_{x=0}^{K} t^x p(X_i=x) \\right)^n\\\\ \n",
    "\\end{align*}$$\n",
    "> \n",
    "> which for $nK+1$ distinct values of $t$, results in $nK+1$ equations with $nK+1$ unknowns, i.e., $p(S=s)$ for $s=0,\\cdots,nK$; however, solving a sytem of $nK+1$ equations is generally an $O(n^3K^3)$ computation, so this formulation alone will not improve upon the initial algorithm.   \n",
    "\n",
    "*The example in this section is inspired by Keith Knight's STA410 [class notes](https://q.utoronto.ca/courses/244990/files?preview=18669454); however, rather than summing over the possible values of the accumulating random variable $(s)$, the summation in the formulation presented here is done only over all possible values of the newly contributed random variable $(x)$, which is an improved formulation since $O\\left(K^2 n^2\\right) < O\\left(K^2 n^3\\right)$;  indeed, the formulation achieves the same algorithmic complexity as a* ***fast Fourier transform*** *because in the discrete case the computation can be restricted to only the arithmetic necessary for the solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOpKFosYygbT"
   },
   "source": [
    "<a name=\"cell-sovling-bigO-Onlogn\"></a>\n",
    "\n",
    "### $O(n\\log(n)) \\text{ and } O(n^{\\log_2 7}) \\approx O(n^{2.81})$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "\"***Big O***\" ***algorithmic complexity*** $O(n \\log n)$ arises from  algorithms leveraging the ***divide and conquer*** nature of [recursion](https://www.google.com/search?q=recursion)\n",
    "\n",
    "```python\n",
    "divNconq_recursion(x):\n",
    "    if stopping_criterion_met:\n",
    "        return small_problem_answer\n",
    "    # otherwise extract `half_of_x` and `rest_of_x` and\n",
    "    return DivnConq_recursion(half_of_x) + DivnConq_recursion(rest_of_x)\n",
    "```\n",
    "\n",
    "which (recursively) reduces a problem into two separate problems, each of half the size as the original until they are small enough to solve rapidly. This results in computationally tractable $O(n\\log n)$ algorithms which are feasible for large $n$ compared to $O(n^2)$. \n",
    "\n",
    "- An input of size $n$ be can recursively split in half $\\log_2 n$ times since definitionally $2^{\\log_2 n} = n$. At each stage of subsequent (recursive) splitting, all of the $n$ elements must still be accessed in order to determine the new splits.  So for each $2^{\\log_2 n}$ splitting stage, $n$ input elements are accessed, and this is what leads to the $O(n\\log n)$ \"***Big O***\" ***algorithmic complexity*** of ***divide and conquer*** recursion. \n",
    "\n",
    "> The famous ***Fast Fourier Transform*** \n",
    "> - $\\tilde x = Ax, \\;A_{jk} = e^{-k\\left(\\frac{j}{n}\\right) 2\\pi i}$\n",
    "> - $x = A^{-1}\\tilde x, \\;A^{-1}_{jk} = \\frac{1}{n}e^{k\\left(\\frac{j}{n}\\right) 2\\pi i}$\n",
    "> is a special matrix-vector multiplication operation that can be computed in an $O(n \\log n)$ recursive ***divide and conquer*** manner, as opposed to the typical $O(n^2)$ computation of standard matrix-vector multiplication.  Thus, if the problem being solved is in some way benefited by making the initial transformation, then the transformation itself will likely not be a limiting computational factor. \n",
    "\n",
    "> Strassen's divide and conquer matrix multiplication algorithm achieves $O(n^{\\log_2 7}) \\approx O(n^{2.81})$ which is a significant imporovement over $O(n^3)$ for moderate $n$.\n",
    "\n",
    "Not all ***divide and conquer*** algorithms are necessarily $O(n \\log n)$. The `fan` summation method from the [Week 2 Programming Assignment]() can be specified as the ***divide and conquer*** algorithm \n",
    "\n",
    "```python\n",
    "def fansum_recursive(x):\n",
    "  '''assumes len(x)=2*p always'''\n",
    "  if len(x)==2:\n",
    "    return sum(x)\n",
    "  return fansum(x_odds)+fansum(x_evens)\n",
    "```\n",
    "\n",
    "which is $O(n \\log n)$ as described above.  But this is because the recursive overhead is actually unnecessary and is equivalent to \n",
    "\n",
    "\n",
    "```python\n",
    "def fansum_iterative(x):\n",
    "  '''assumes len(x)=2*p always'''\n",
    "  if len(x)==2:\n",
    "    return sum(x)\n",
    "  return fansum(x_odds+x_evens)\n",
    "```\n",
    "\n",
    "The `fan` method then is a ***divide and conquer*** algorithm which doesn't reduce a problem into two smaller problems that are half as small; but, rather, it reduces a problem of size $n$ to a single new problem of size $n/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pcEfDyWn1Kq"
   },
   "outputs": [],
   "source": [
    "def fansum_recursive(x):\n",
    "  '''assumes len(x)=2*p always'''\n",
    "  if len(x)==2:\n",
    "    return sum(x)\n",
    "  # slightly different but generally equivalent formulation\n",
    "  return fansum_recursive(x[:int(len(x)/2)])+fansum_recursive(x[int(len(x)/2):])\n",
    "\n",
    "def fansum_iterative(x):\n",
    "  '''assumes len(x)=2*p always'''\n",
    "  if len(x)==2:\n",
    "    return sum(x)\n",
    "   # slightly different but generally equivalent formulation   \n",
    "  return fansum_iterative(x[:int(len(x)/2)]+x[int(len(x)/2):])\n",
    "\n",
    "x = np.linspace(1,2,2**5)\n",
    "# these are mathematically equivalent methods\n",
    "np.isclose(fansum_recursive(x), fansum_iterative(x))\n",
    "# but the iterative form is $O(n)$ and avoids \"split into to half problems\" \n",
    "# overhead entailed in the recursive divide and conquer version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18dTILOQoRhV"
   },
   "outputs": [],
   "source": [
    "# O(n log n) \"split into to half problems\"  divide and conquer\n",
    "%timeit fansum_recursive(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpWGhVyfoRkO"
   },
   "outputs": [],
   "source": [
    "# O(n) \"sum to reduce\" direct iterative method with no log(n) overhead\n",
    "%timeit fansum_iterative(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pp4lF0EjgB8f"
   },
   "source": [
    "<a name=\"cell-sovling-direct-methods\"></a>\n",
    "\n",
    "## 4.1 Direct Methods for $X\\hat \\beta \\approx y$ by solving for $\\hat \\beta$ in $\\overset{A\\, x}{X^TX \\beta} = \\overset{b}{X^T y}$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "## $\\hat \\beta$ solving $\\;X^TX\\hat \\beta = X^Ty\\;$ provides the ***least squares*** approximation $X\\hat \\beta \\approx  y$ \n",
    "\n",
    "\n",
    "***Direct methods*** execute a set sequence of predefined deterministic computational steps to arrive at the exact solution (to the limits of available precision) to a computational problem. The various `sum` and `variance` computations considered in the `STA410_W24_Week4_Homework_AdditionVariance.ipynb` are examples of ***direct methods***. The ***Gram-Schmidt orthogonalization*** and ***Cholesky decomposition*** considered in the `STA410_W24_Week5_Extra_LinearAlgebraAlgorithms.ipynb` are examples of ***direct methods***. And the computation of the ***SVD*** is a ***direct methods***. While we have not seen exactly what the ***SVD*** algorithm is, suffice it to say that is is similar in character to the ***Gram-Schmidt orthogonalization*** and ***Cholesky decompositions***.\n",
    "\n",
    "Here are some new examples of ***direct methods***\n",
    "\n",
    "1. `np.linalg.inv(X.T@X) @ X^Ty # Analytical solution`\n",
    "2. `np.linalg.solve(X.T@X, X^Ty) # Gaussian elimination`\n",
    "3. `# QR decomposition based least squares estimates`\n",
    "\n",
    "   `Q,R = np.linalg.qr(X); scipy.linalg.solve_triangular(R, Q.T@y)` \n",
    "4. `C = np.linalg.cholesky(X.T@X) # Cholesky decomposition based least squares estimates`\n",
    "\n",
    "   `scipy.linalg.solve_triangular(C, scipy.linalg.solve_triangular(C, X.T@y, lower=True))`\n",
    "5. `np.linalg.lstsq(X, y)  # SVD based least squares estimates` \n",
    "\n",
    "used to solve for the least squares solution $\\hat \\beta$ solving $X^TX\\hat \\beta = X^T y$ (or more generally for $x$ solving $Ax=b$ when a solution exists). These are ***direct methods*** because each each algorithmically computes the prescribed steps of a linear algebra prescription that computates $\\hat \\beta$.\n",
    "\n",
    "> All of these methods (for $X_{n\\times n}$ with $n>m$) are $O(nm^2)$; although, importantly, they will have different computational run times for a given $n$ and $m$ since $M$ in $f(n,m)\\leq Mnm^2$ can be (and is) different for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6QkYPj_Juxk"
   },
   "source": [
    "<a name=\"cell-sovling-direct-methods-big0\"></a>\n",
    "\n",
    "## 4.1.0 \"Big O\" Algorithmic Complexity ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "The \"***Big O***\" ***algorithmic complexity*** for $n>m$ of all of the ***direct methods*** above is $O(nm^2)$. \n",
    "\n",
    "- `np.linalg.inv(X.T@X) @ X^Ty` is $(X^T@X)^{-1} (X_{n\\times m}^Ty) = O(nm^2)$ since this term dominates the operations together $O(nm^2) + O(m^3) + O(nm) + O(m^2)$ \n",
    "  - `X.T@X` matrix-matrix multiplication is $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$ \n",
    "  - `np.linalg.inv(A_mxm)` [matrix inversion](http://gregorygundersen.com/blog/2020/12/09/matrix-inversion/) and even [***upper triangular matrix*** inversion](https://math.stackexchange.com/questions/92068/fast-inversion-of-a-triangular-matrix) is $X_{n\\times m}^Ty = O(m^3)$ \n",
    "  - `X.T@y` matrix-vector multiplication is $X_{n\\times m}^Ty = O(nm)$\n",
    "  - `Ainv_mxm @ b_mx1` is $O(m^2)$.\n",
    "\n",
    "- `np.linalg.solve(X.T@X, X.T@y)` is again $O(nm^2)$ as $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$ will again dominate $X_{n\\times m}^Ty = O(nm)$ and the other required $O(m^3)$ and $O(m^2)$ oprations\n",
    "  - `np.linalg.solve(A_mxm, b)` is $O(m^3)$ since (as shown previously) it requires the sequential application of \n",
    "\n",
    "    1. ***Gaussian elimination*** with a required operation count of $2 \\times \\left(\\frac{m(m+1)(2m+1)}{6} - m + \\frac{m(mm+1)}{2} - m \\right) = O(m^3)$ \n",
    "\n",
    "    2. ***backward substitution*** with a required operation count of $m^2 = O(m^2)$ \n",
    "\n",
    "- The ***QR decomposition*** `Q,R = np.linalg.qr(X)` using [***Householder transformations***](https://math.stackexchange.com/questions/501018/what-is-the-operation-count-for-qr-factorization-using-householder-transformatio) which are more numerically accurate than the ***(modified) Graham-Schmidt procedure*** is $O(nm^2)$ \n",
    "  - `scipy.linalg.solve_triangular(R, Q.T@y)` is again $m^2 = O(m^2)$ \n",
    "\n",
    "- The ***Cholesky decomposition*** `C = np.linalg.cholesky(X.T@X)` is a ***Gaussian elimination*** like process of $O(m^3)$, but the prerequesite $X_{n\\times m}^TX_{n\\times m}$ computation is $O(nm^2)$\n",
    "\n",
    "  - `scipy.linalg.solve_triangular(C, scipy.linalg.solve_triangular(C, X.T@y, lower=True))` are each $m^2 = O(m^2)$ \n",
    "\n",
    "- The [standard computation](https://www.mathworks.com/matlabcentral/answers/420057-what-is-the-complexity-of-matlab-s-implementation-of-svd) of the ***SVD*** [implemented](https://scicomp.stackexchange.com/questions/6979/how-is-the-svd-of-a-matrix-computed-in-practice) by `np.linalg.lstsq(X, y)` is $O(nm^2)$\n",
    "\n",
    "  - The subsquent $\\hat \\beta = V (D^{+})^2 V^T X^T y$ is $O(\\max(m^3,nm))$ depending on if $VV^T$ or $X^T y$ is more computationally expensive\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
